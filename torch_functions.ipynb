{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compare performance of PyTorch in the CPU Vs in the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "# torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "# np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "# np_rand2 = torch.rand(100, 100, 100, 100)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# elapsed_time = end_time - start_time\n",
    "# #print(f\"{elapsed_time:.8f}\")\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# rand = np.multiply(np_rand1, np_rand2)\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# #print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Conclusion:\n",
    "When tensor is a big dimension matrix (3D or more), operations on the GPU are better/faster than on the CPU</h5> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some Pytorch functions: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Create Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = torch.tensor([[1 , 2 , 3, 4],[5 , 6 , 7.25, 8]]) \n",
    "\n",
    "tab_randint1=torch.randint(10, (3, )) #Create a 1D tensor with 3 columns \n",
    "tab_randint2=torch.randint(2, (3, 4)) #Create a tensor (3,4) with integer that never exceed 2 excluded (so it has 0 and 1 values)\n",
    "tab_randint3 = torch.randint(8, (3, 4, 5)) #Create a 3D tensor with 3 blocks, 4 rows and 5 columns \n",
    "\n",
    "\n",
    "\n",
    "tab_rand1 = torch.rand(5) #Create a 1D Matrix: 5 columns (float)\n",
    "tab_rand2 = torch.rand(2,5) #Create a 2D Matrix: 2 rows, each row has 5 columns (float)\n",
    "tab_rand3 = torch.rand(3,2,5) #Create a 3D Matrix: 3 blocks, each block has 2 rows, each row has 5 columns (float)\n",
    "tab_rand4 = torch.rand(2, 3,2,5) #Create a 4D Matrix: 2 BigBlocks, each BigBlock has 3 blocks, each block has 2 rows, each row has 5 columns (float)\n",
    "\n",
    "tab_empty = torch.empty(2,4) #It fills a tensor with really small values (generated randomnly) RESPECTING THE DIMENSION PASSED AS PARAMETERS\n",
    "\n",
    "tab_zeros = torch.zeros(4,4) #It fills a tensor with zeros\n",
    "tab_ones = torch.ones(3,5) #It fills a tensor with ones\n",
    "\n",
    "tab_identity_matrix = torch.eye(3)\n",
    "\n",
    "tab_arange = torch.arange(start=5, end=15, step=3) #In this example, it creates: (tensor([5,8,11,14]))\n",
    "tab_linspace = torch.linspace(start=3, end=10, steps=5) #In this example, it creates: (tensor([3, 4.75 , 6.5 , 8.25 , 10]))\n",
    "#NB: 'step' parameter in arange != 'steps' parameter in linspace. \n",
    "\n",
    "tab_logspace = torch.logspace(start=-5, end=5, steps=3, base=10) #In this example, it creates: tensor([1.0000e-05, 1.0000e+00, 1.0000e+05])\n",
    "#NB: The default base we use is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Modify/Create Tensor: _uses tensor as parameter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_empty_like = torch.empty_like(tab) #It fills a tensor with really small values (generated randomnly) RESPECTING THE DIMENSION OF TAB\n",
    "\n",
    "tab_tril = torch.tril(tab) #To keep the lower part of the tensor 'tab'\n",
    "tab_triu = torch.triu(tab) #To keep the upper part of the tensor 'tab'\n",
    "#NB: triu starts from the second row and is the one like a staircase (Converting to 0 one column per row)\n",
    "\n",
    "temp_proba= torch.tensor([[0.1, 0.3, 0.6], [0.2, 0.5, 0.3]])\n",
    "tab_proba = torch.multinomial(temp_proba, num_samples=5, replacement=True) \n",
    "# Replace the size of the column with 'num_samples' and the values of the column with an index integer\n",
    "#NB1: Sum of each row in tensor for multinommust be equal to 1\n",
    "#NB2: replacement=True means that the index is returned to the pool after it has been selected\n",
    "\n",
    "tab_concatenate = torch.cat( ( tab, torch.tensor([[66], [99]]) ), dim=1) # Concatenate two tensors (so both tensors must be same Matrix Dimension)\n",
    "# NB: 'dim' is the dimension over which the tensors are concatenated. dim=0 is for rows, dim=1 is for columns\n",
    "\n",
    "\n",
    "temp_masked = torch.tensor([[0, 1, 0, 3],[0 , 0 , 0, 22] ]) \n",
    "tab_masked = tab.masked_fill(temp_masked == 0, float('-inf')) #NB: temp_masked MUST BE the same dimension as tab\n",
    "\n",
    "tab_exp = torch.exp(tab)\n",
    "\n",
    "temp_transpose = torch.rand(2,3,4)\n",
    "tab_transpose = temp_transpose.transpose(0, 2) # With this example, We swapped 'dim0' with 'dim2'. So tab_transpose become of (4, 3, 2) \n",
    "\n",
    "\n",
    "tensor1 = torch.rand(2,3,4)\n",
    "tensor2 = torch.rand(2,3,4)\n",
    "tensor3 = torch.rand(2,3,4)\n",
    "tab_stacktensors = torch.stack([tensor1, tensor2, tensor3]) # tab_stacktensors stacked three cubes into one tensor. So we get: (3, 2, 3, 4)\n",
    "# NB: torch.stack add a new dimension, while torch.cat change the size of one dimension \n",
    "\n",
    "\n",
    "tab_A= torch.rand(2,3)\n",
    "tab_B= torch.rand(3,4)\n",
    "tab_multiplication = tab_A @ tab_B #Can also be written: tab_multiplication = torch.matmul(tab_A, tab_B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Network functions: </h3>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Linear function:**     \n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "- $x$ is the input tensor.\n",
    "- $W$ is the weight matrix, which is learned during training.\n",
    "- $b$ is the bias vector, which is also learned during training.\n",
    "- $y$ is the output of the linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Softmax Function**\n",
    "\n",
    "For example, we have `tensor([3.25, 5, 1.1])`.\n",
    "\n",
    "This function performs the following steps:\n",
    "\n",
    "a. Exponentiate each element:\n",
    "   - `exp(3.25) = 25.79`\n",
    "   - `exp(5) = 148.41`\n",
    "   - `exp(1.1) = 3.00`\n",
    "\n",
    "b. Sum the results:\n",
    "   - `Sum = 25.79 + 148.41 + 3.00 = 177.2`\n",
    "\n",
    "c. Create a new tensor `[X1, X2, X3]`:\n",
    "   - `X1 = 25.79 / 177.2 = 0.14`\n",
    "   - `X2 = 148.41 / 177.2 = 0.83`\n",
    "   - `X3 = 3.00 / 177.2 = 0.01`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_softmax = torch.tensor([[3.25, 5, 1.1], [4, 4, 4]])\n",
    "tab_softmax = F.softmax(temp_softmax, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Embedded Vectors: </h3>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important !! EACH ROW of embedded vector is for only ONE TOKEN (for example first row of embedding_layer is for letter a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_values_as_key = {char: idx for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "\n",
    "word = \"cat\" # Input: A sequence of character tokens (e.g., a word \"cat\")\n",
    "encoded_word = torch.tensor([dict_values_as_key[c] for c in word])\n",
    "\n",
    "\n",
    "vocab_size = len(dict_values_as_key)  # Number of unique characters\n",
    "embedding_dim = 5  # Size of each embedding vector (Hyperparameter that can be changed)\n",
    "\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim) # Create an embedding layer (26,5)\n",
    "\n",
    "char_embeddings = embedding_layer(encoded_word) # Associate each embedding with a token (and since they are the same size with vocab_size, every character will have an embedding associated to it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resources:</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>loss formula: </b> - ln(1/vocab_size) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
