{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compare performance of PyTorch in the CPU Vs in the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "# torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "# np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "# np_rand2 = torch.rand(100, 100, 100, 100)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# elapsed_time = end_time - start_time\n",
    "# #print(f\"{elapsed_time:.8f}\")\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# rand = np.multiply(np_rand1, np_rand2)\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# #print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Conclusion:\n",
    "When tensor is a big dimension matrix (3D or more), operations on the GPU are better/faster than on the CPU</h5> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some Pytorch functions: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Create Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = torch.tensor([[1 , 2 , 3, 4],[5 , 6 , 7.25, 8]]) \n",
    "\n",
    "\n",
    "tab_rand1 = torch.rand(4) #Create a 1D Matrix: 4 columns\n",
    "tab_rand2 = torch.rand(2,4) #Create a 2D Matrix: 2 rows, each row has 4 columns\n",
    "tab_rand3 = torch.rand(3,2,4) #Create a 3D Matrix: 3 blocks, each block has 2 rows, each row has 4 columns\n",
    "tab_rand4 = torch.rand(2, 3,2,4) #Create a 4D Matrix: 2 BigBlocks, each BigBlock has 3 blocks, each block has 2 rows, each row has 4 columns\n",
    "\n",
    "tab_empty = torch.empty(2,4) #It fills a tensor with really small values (generated randomnly) RESPECTING THE DIMENSION PASSED AS PARAMETERS\n",
    "\n",
    "tab_zeros = torch.zeros(4,4) #It fills a tensor with zeros\n",
    "tab_ones = torch.ones(3,5) #It fills a tensor with ones\n",
    "\n",
    "tab_identity_matrix = torch.eye(3)\n",
    "\n",
    "tab_arange = torch.arange(start=5, end=15, step=3) #In this example, it creates: (tensor([5,8,11,14]))\n",
    "tab_linspace = torch.linspace(start=3, end=10, steps=5) #In this example, it creates: (tensor([3, 4.75 , 6.5 , 8.25 , 10]))\n",
    "#NB: 'step' parameter in arange != 'steps' parameter in linspace. \n",
    "\n",
    "tab_logspace = torch.logspace(start=-5, end=5, steps=3, base=10) #In this example, it creates: tensor([1.0000e-05, 1.0000e+00, 1.0000e+05])\n",
    "#NB: The default base we use is 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Modify/Create Tensor: _uses tensor as parameter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3764, 0.1711, 0.1102, 0.7713],\n",
      "          [0.4673, 0.2592, 0.4648, 0.1450],\n",
      "          [0.7245, 0.7269, 0.4129, 0.1108]],\n",
      "\n",
      "         [[0.9967, 0.4481, 0.9640, 0.9010],\n",
      "          [0.8054, 0.5749, 0.2991, 0.3693],\n",
      "          [0.1920, 0.0287, 0.3895, 0.9267]]],\n",
      "\n",
      "\n",
      "        [[[0.0867, 0.6558, 0.6796, 0.3140],\n",
      "          [0.5783, 0.5470, 0.8312, 0.5556],\n",
      "          [0.6364, 0.1681, 0.8288, 0.4404]],\n",
      "\n",
      "         [[0.7168, 0.5546, 0.9273, 0.5368],\n",
      "          [0.5675, 0.7044, 0.4230, 0.2560],\n",
      "          [0.1817, 0.4280, 0.4209, 0.8573]]],\n",
      "\n",
      "\n",
      "        [[[0.3514, 0.7073, 0.6781, 0.5810],\n",
      "          [0.6884, 0.9038, 0.7793, 0.8443],\n",
      "          [0.6810, 0.3752, 0.8630, 0.4164]],\n",
      "\n",
      "         [[0.0711, 0.5326, 0.7875, 0.3982],\n",
      "          [0.6665, 0.0098, 0.6209, 0.7962],\n",
      "          [0.5415, 0.5132, 0.0687, 0.4714]]]])\n"
     ]
    }
   ],
   "source": [
    "tab_empty_like = torch.empty_like(tab) #It fills a tensor with really small values (generated randomnly) RESPECTING THE DIMENSION OF TAB\n",
    "\n",
    "tab_tril = torch.tril(tab) #To keep the lower part of the tensor 'tab'\n",
    "tab_triu = torch.triu(tab) #To keep the upper part of the tensor 'tab'\n",
    "#NB: triu starts from the second row and is the one like a staircase (Converting to 0 one column per row)\n",
    "\n",
    "temp_proba= torch.tensor([[0.1, 0.3, 0.6], [0.2, 0.5, 0.3]])\n",
    "tab_proba = torch.multinomial(temp_proba, num_samples=5, replacement=True) \n",
    "# Replace the size of the column with 'num_samples' and the values of the column with an index integer\n",
    "#NB1: Sum of each row in tensor for multinommust be equal to 1\n",
    "#NB2: replacement=True means that the index is returned to the pool after it has been selected\n",
    "\n",
    "tab_concatenate = torch.cat( ( tab, torch.tensor([[66], [99]]) ), dim=1) # Concatenate two tensors (so both tensors must be same Matrix Dimension)\n",
    "# NB: 'dim' is the dimension over which the tensors are concatenated. dim=0 is for rows, dim=1 is for columns\n",
    "\n",
    "\n",
    "temp_masked = torch.tensor([[0, 1, 0, 3],[0 , 0 , 0, 22] ]) \n",
    "tab_masked = tab.masked_fill(temp_masked == 0, float('-inf')) #NB: temp_masked MUST BE the same dimension as tab\n",
    "\n",
    "tab_exp = torch.exp(tab)\n",
    "\n",
    "temp_transpose = torch.rand(2,3,4)\n",
    "tab_transpose = temp_transpose.transpose(0, 2) # With this example, We swapped 'dim0' with 'dim2'. So tab_transpose become of (4, 3, 2) \n",
    "\n",
    "\n",
    "tensor1 = torch.rand(2,3,4)\n",
    "tensor2 = torch.rand(2,3,4)\n",
    "tensor3 = torch.rand(2,3,4)\n",
    "tab_stacktensors = torch.stack([tensor1, tensor2, tensor3]) # tab_stacktensors stacked three cubes into one tensor. So we get: (3, 2, 3, 4)\n",
    "# NB: torch.stack add a new dimension, while torch.cat change the size of one dimension \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
